{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25869d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99ab47",
   "metadata": {},
   "source": [
    "Based on Piotr Czapla:  \n",
    "https://github.com/PiotrCzapla/course22p2/blob/main/nbs/12_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from miniai.learner import Callback\n",
    "from miniai.learner import MetricsCB, master_bar, progress_bar, to_cpu\n",
    "\n",
    "import numpy as np\n",
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import time\n",
    "\n",
    "# Simplified from Piotr\n",
    "\n",
    "def _format_stats(stats):\n",
    "    return f\"{stats.mean():.2f} Â± {stats.std():.2f}\"\n",
    "\n",
    "class TimeItSimpleCB(Callback):\n",
    "    def __init__(self): self.reset()\n",
    "    def tick(self): return time.time()\n",
    "         \n",
    "    def reset(self):\n",
    "        self.start = self.tick()\n",
    "        self.batches = {True:[], False:[]}\n",
    "        self.epochs = {True:[], False:[]}\n",
    "        \n",
    "    def before_fit(self, learn): self.reset()\n",
    "        \n",
    "    def before_batch(self, learn):\n",
    "        self.batches[learn.training].append(self.tick())\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        self.batches[learn.training][-1] = self.tick() - self.batches[learn.training][-1]\n",
    "    \n",
    "    def before_epoch(self, learn):\n",
    "        self.epochs[learn.training].append(self.tick())\n",
    "    \n",
    "    def after_epoch(self, learn):\n",
    "        self.epochs[learn.training][-1] = self.tick() - self.epochs[learn.training][-1]\n",
    "    \n",
    "    def after_fit(self, learn):\n",
    "        self.total = self.tick() - self.start\n",
    "        self.batches = {k:np.array(v) for k,v in self.batches.items()}\n",
    "        self.epochs = {k:np.array(v) for k,v in self.epochs.items()}\n",
    "        self.print_stats()\n",
    "    \n",
    "    def print_stats(self): \n",
    "        print(f\"Fit {len(self.epochs[True])} in: {self.total:.2f}s, {_format_stats(self.epochs[True])}s per epoch, {_format_stats(self.batches[True])}s per batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Same as Piotr\n",
    "\n",
    "# if spawn is not possible, we can cache the entire dataset in memory after transformations\n",
    "def _with_features(ds):\n",
    "    setattr((l:=fc.L(ds)), 'features', ds.features)\n",
    "    return l \n",
    "\n",
    "class CachedDS(dict): \n",
    "    def __repr__(self): return \"{ \"+\", \".join([f'{k}: (#{len(v)})' for k,v in self.items()])+\" }\"\n",
    "    def __str__(self): return repr(self)\n",
    "\n",
    "def cache_dataset_as_dict(dd): return CachedDS({dsn: _with_features(ds) for dsn,ds in dd.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Same as Piotr\n",
    "\n",
    "class LazyProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): \n",
    "        self.plot = plot\n",
    "        self.count = 0\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.dev_losses = []\n",
    "        self.losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(map(str, d.values())), table=True)\n",
    "        if self.plot:\n",
    "            for l in self.dev_losses: self.losses.append(to_cpu(l))\n",
    "            self.dev_losses=[]\n",
    "            #print(self.losses[:10])\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])\n",
    "        \n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        #learn.dl.comment = f'{learn.loss:.3f}' # not sure how it is being used\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.dev_losses.append(learn.loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc608542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fmussari/mambaforge/envs/fastaiv3/lib/python3.9/site-packages/nbdev/export.py:54: UserWarning: Notebook '/mnt/m/2023-02_fastai-course22p2/course22p2/nbs/challenge/2023-02. Curricular MixIn.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4346b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
