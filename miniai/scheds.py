# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12b_schedulers.ipynb.

# %% auto 0
__all__ = ['mai_hp_map', 'smooth_gamma', 'sched_exp', 'sched_cos', 'sched_poly', 'sched_fastai_exp', 'SchedCos', 'SchedExp',
           'SchedExpFastai', 'SchedLin', 'SchedNo', 'SchedPoly', 'CombineScheds', 'CombinedCos', 'detuplify_pg',
           'set_item_pgs', 'get_item_pgs', 'mapping', 'format_start', 'ParamScheduler', 'OneCycleSched', 'FlatCosSched']

# %% ../nbs/12b_schedulers.ipynb 2
import torch
import fastcore.all as fc
import numpy as np
from torch import tensor
import math

from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .init import *
from .sgd import *

# %% ../nbs/12b_schedulers.ipynb 22
def smooth_gamma(gamma):
    gamma = 1 + 7 * gamma
    return 0.000268*gamma**3-0.005078*gamma**2+0.039773*gamma+0.864643

def sched_exp(start, end, gamma, pos): 
    assert gamma>=0 and gamma<=1
    pos,final_pos = pos*100, 100
    g = smooth_gamma(gamma)
    theta = g**final_pos / final_pos
    return (start-end)*g**pos + (end-start)*theta*pos + end

# %% ../nbs/12b_schedulers.ipynb 25
def sched_cos(start, end, pos): return end + (start - end)/2 * (1 + math.cos(pos*math.pi))
def sched_poly(start, end, power, pos): return start + (end - start) * pos ** power
def sched_fastai_exp(start, end, pos): return start * (end/start) ** pos

# %% ../nbs/12b_schedulers.ipynb 26
class _Annealer:
    def __init__(self, f, start, end): fc.store_attr()
    def __call__(self, pos): return self.f(start=self.start, end=self.end, pos=pos)

# %% ../nbs/12b_schedulers.ipynb 27
def SchedCos(start, end): return _Annealer(sched_cos, start, end)
def SchedExp(start, end, gamma): return _Annealer(partial(sched_exp, gamma=gamma), start, end)
def SchedExpFastai(start, end): return _Annealer(sched_fastai_exp, start, end)
def SchedLin(start, end): return _Annealer(partial(sched_exp, gamma=1), start, end)
def SchedNo(start): return _Annealer(partial(sched_poly, power=0), 0, start)
def SchedPoly(start, end, power): return _Annealer(partial(sched_poly, power=power), start, end)

# %% ../nbs/12b_schedulers.ipynb 32
class CombineScheds:
    "Combine `scheds` according to `pcts` in one class"
    def __init__(self, pcts, scheds):
        assert sum(pcts) == 1.
        assert (len(pcts)>=2) and (len(pcts)==len(scheds))
        pcts = tensor([0] + pcts)
        assert torch.all(pcts >= 0)
        self.scheds = scheds
        self.pcts = torch.cumsum(pcts, 0)
        self.pct_lim = len(self.pcts) - 2
    
    def _get_idx_apos(self, pos):
        idx = min((pos >= self.pcts).nonzero().max(), self.pct_lim)
        actual_pos = (pos-self.pcts[idx]) / (self.pcts[idx+1]-self.pcts[idx])
        return idx, actual_pos
        
    def __call__(self, pos):
        idx, actual_pos = self._get_idx_apos(pos)
        return self.scheds[idx](pos=actual_pos.item())

    @property
    def start(self): return self.scheds[0].start

    @start.setter
    def start(self, value): self.scheds[0].start = value

# %% ../nbs/12b_schedulers.ipynb 33
class CombinedCos(CombineScheds):
    "Return a scheduler with cosine annealing from `start`→`middle` & `middle`→`end`"
    def __init__(self, pct, start, middle, end):
        super().__init__([pct,1-pct], [SchedCos(start, middle), SchedCos(middle, end)])

# %% ../nbs/12b_schedulers.ipynb 35
# hyperparameters map
mai_hp_map = {'lr': 'lr', 'momentum': 'mom', 'betas__0': 'mom', 'betas__1': 'sqr_mom'}

# %% ../nbs/12b_schedulers.ipynb 36
# based on fastai's functions (some are the same as fastai's)
def detuplify_pg(d):
    res = {}
    for k,v in d.items():
        if k == 'params': continue
        if fc.is_listy(v): 
            res.update(**{f'{k}__{i}': v_ for i,v_ in enumerate(v)})
        else: res[k] = v
    return res

def set_item_pgs(pgs, k, vs):
    for v, pg in zip(vs, pgs):
        if '__' not in k: pg[k] = v
        else:
            name,idx = k.split('__')
            pg[name] = tuple(v if i==int(idx) else pg[name][i] for i in fc.range_of(pg[name]))

def get_item_pgs(pgs, k):
    res = []
    for pg in pgs:
        if '__' not in k: res.append(pg[k])
        else:
            name,idx = k.split('__')
            res.append(pg[name][int(idx)])
    return np.array(res)

def mapping(pgs, scheds):
        n_pgs = len(pgs)
        fwd_map = {k: v for k,v in mai_hp_map.items() if k in detuplify_pg(pgs[0]).keys()}
        bwd_map = {k: v for v,k in fwd_map.items() if k in scheds.keys()}
        return fwd_map, bwd_map

def format_start(v, n_pgs):
    v = [v] if isinstance(v, (int, float)) else v
    v = list(v)
    if len(v)==1: v = v * n_pgs
    assert len(v) == n_pgs, f"Trying to set {len(v)} values, but there are {n_pgs} parameter groups."
    return np.array(v)

# %% ../nbs/12b_schedulers.ipynb 38
class ParamScheduler:
    "Scheduler for multiple hyperparameters and groups"
    def __init__(self, scheds, total_steps, optimizer):
        self.opt = optimizer
        self.final_step = total_steps - 1
        self.scheds = scheds
        self.pgs = optimizer.param_groups
        self._step_count = 0
        self._init_sched()
        self.step()

    def step(self):
        pct_train = self._step_count / self.final_step
        if pct_train <= 1: 
            for k, sched in self.scheds.items():
                vs = sched(pct_train)
                set_item_pgs(self.pgs, self.bwd_map[k], vs)
        self._step_count += 1

    def _init_sched(self):
        self.fwd_map, self.bwd_map = mapping(self.pgs, self.scheds)
        n_pgs = len(self.pgs)
        for k, sched in self.scheds.items():
            if not sched.start: sched.start = get_item_pgs(self.pgs, self.bwd_map[k])
            else: sched.start = format_start(sched.start, n_pgs)
            if isinstance(sched, CombineScheds):
                for sc in sched.scheds[1:]: sc.start = format_start(sc.start, n_pgs)

# %% ../nbs/12b_schedulers.ipynb 40
class OneCycleSched(ParamScheduler):
    "OneCycle ParamScheduler"
    def __init__(self, optimizer, total_steps, lr_max=None, div=25., div_final=1e5, pct_start=0.25, moms=None):
        fc.store_attr()
        self.pgs = optimizer.param_groups
        self._init_onecycle_sched()
        super().__init__(self.scheds, total_steps, optimizer)
    
    def _init_onecycle_sched(self):
        if not self.lr_max: self.lr_max = get_item_pgs(self.pgs, 'lr')
        if not self.moms: self.moms = (0.95, 0.85, 0.95)
        self.scheds = {'lr':  CombinedCos(self.pct_start, self.lr_max/self.div, self.lr_max, self.lr_max/self.div_final),
            'mom': CombinedCos(self.pct_start, *self.moms)}

# %% ../nbs/12b_schedulers.ipynb 41
class FlatCosSched(ParamScheduler):
    "FlatCos ParamScheduler"
    def __init__(self, optimizer, total_steps, lr=None, div_final=1e5, pct_start=0.75):
        fc.store_attr()
        self.pgs = optimizer.param_groups
        self._init_flatcos_sched()
        super().__init__(self.scheds, total_steps, optimizer)
    
    def _init_flatcos_sched(self):
        if not self.lr: self.lr = get_item_pgs(self.pgs, 'lr')
        self.scheds = {'lr':  CombinedCos(self.pct_start, self.lr, self.lr, self.lr/self.div_final)}
